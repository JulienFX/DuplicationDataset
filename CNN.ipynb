{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a0411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network from scratch \n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01160a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mnist_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7cff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data) # transform our csv into array \n",
    "lines, cols = data.shape # lines = 42000 / columns = 785 bcz the images are 28x28=784 pixels +1 with the beggining cols of label\n",
    "np.random.shuffle(data) # because the train data are possibly sorted with a sequences of 0 then 1 then 2 and so on \n",
    "# we shuffe the dataset in order to avoid overfitting\n",
    "data_validation = data[0:1000].T # Data transposition is performed in this particular context to facilitate \n",
    "# the manipulation of examples and features. Some machine learning algorithms and libraries prefer \n",
    "# this matrix representation with examples in columns rather than rows.\n",
    "# data validation variable is used to evaluate and confirm the performance of our future model while training\n",
    "label_val = data_validation[0]\n",
    "result_val = data_validation[1:cols]\n",
    "result_val = result_val / 255. # we normalise between 0 and 1 in order to facilitate the process of learning\n",
    "# The dot is here to indicate that the division should be done with a decimal precision\n",
    "\n",
    "data_train = data[1000:lines].T\n",
    "label_train = data_train[0] # we did the transpose so the first line is compose only by label\n",
    "result_train = data_train[1:cols]\n",
    "result_train = result_train / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88aa1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params() :\n",
    "    # W => weights / b => bias ... In the beginning both are generated randomly \n",
    "    # 1st param : nb neurons on the actual layer / 2st param : nb neurons of the previous layer\n",
    "    neuronsLayerOne = 10 \n",
    "    neuronsLayerTwo = 10\n",
    "    W1 = np.random.rand(neuronsLayerOne,cols-1) -0.5 # -1 => to understand refer to what we said on the line of creation of cols\n",
    "    b1 = np.random.rand(neuronsLayerOne,1) - 0.5 # which means for each neurons we generate a random number which will corresponding to the bias\n",
    "    # bias doesn't have to be necessary between 0 and 1 ! \n",
    "    W2 = np.random.rand(neuronsLayerTwo,neuronsLayerOne) -0.5\n",
    "    b2 = np.random.rand(neuronsLayerTwo,1) -0.5\n",
    "    # - 0.5 is in reality not necessary but bcz these params are gonna be use in softmax function \n",
    "    # which include exponential if we've got a too big number, the exponential will be too hard to compute for the computer \n",
    "    # In other words - 0.5 is only here due to computer's performance \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLu(Z) : #activation function non linear \n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z) : \n",
    "    return np.exp(Z)/sum(np.exp(Z))\n",
    "\n",
    "def deriv_ReLu(Z) :\n",
    "    return Z>0\n",
    "\n",
    "def forward_propagation(W1, b1, W2, b2, X) : # X is the input data (pixels 1 image) / Z -> unactivated layer / A -> activated \n",
    "    # forward propagation is when you take images and run them through the CNN / It's a linear transformation\n",
    "    Z1 = W1.dot(X)+b1 # X -> several lines and 1 column due to the transpose ! (l,c) x (l,c)  W1 -> l : 1 c : 10 \n",
    "    # we tend to represent layer of CNN in line but we do not need to adapt this in our line of code bcz it's not important\n",
    "    A1 = ReLu(Z1)\n",
    "    Z2 = W2.dot(A1)+b2\n",
    "    A2 = softmax(Z2) # final result \n",
    "    # final result Y of our CNN will be score associated to each prediction such as this\n",
    "    # [0 0.05 0.7 0.01 0.09 0.1 0 0.05 0 0] = total equals to 1 such as a distribution function !\n",
    "    'RESUME : \n",
    "    'Forward propagation for image is taking all the pixels of the image and applying them\n",
    "    'some operations : \n",
    "    'SUM (Entry pixels X Weight + bias)\n",
    "    'Non linearity => activation function (ReLu and softmax here)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y) : # function that will take the label of our data and for each label transform it into an encoded matrix \n",
    "    # example : 0 : 100000000 / 4 : 0000100000 / 9 : 0000000001 ...\n",
    "    labelsNb = 10 # 0 1 2 3 4 5 6 7 8 9\n",
    "    one_hot_Y = np.zeros((Y.size, labelsNb)) # to specify tuple we put () / Y.size => number of values of labels that we pass to the function \n",
    "    one_hot_Y[np.arange(Y.size),Y] = 1 \n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_propagation(Z1, A1, Z2, A2, W2, X, Y) : \n",
    "    m = Y.size # equals line\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y # value of the error \n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2) # average of the absolute error \n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1) #TO define \n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    'Documentation'\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha): \n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2  \n",
    "    'Documentation'\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefe9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b95e626",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,784) and (59000,) not aligned: 784 (dim 1) != 59000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[0;32m      9\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m init_params()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 11\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[0;32m     13\u001b[0m     W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(W1, b1, W2, b2, X) : \u001b[38;5;66;03m# X is the input data (pixels 1 image) / Z -> unactivated layer / A -> activated \u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# forward propagation is when you take images and run them through the CNN / It's a linear transformation\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mW1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb1 \u001b[38;5;66;03m# X -> several lines and 1 column due to the transpose ! (l,c) x (l,c)  W1 -> l : 1 c : 10 \u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# we tend to represent layer of CNN in line but we do not need to adapt this in our line of code bcz it's not important\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m ReLu(Z1)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (10,784) and (59000,) not aligned: 784 (dim 1) != 59000 (dim 0)"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(label_train,result_train,  0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "169c678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9955d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) -0.5\n",
    "    b1 = np.random.rand(10, 1)  -0.5\n",
    "    W2 = np.random.rand(10, 10) -0.5\n",
    "    b2 = np.random.rand(10, 1) -0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "627a00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b331b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[1 1 1 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.11067796610169492\n",
      "Iteration:  10\n",
      "[1 1 1 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.1477627118644068\n",
      "Iteration:  20\n",
      "[1 2 1 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.1709322033898305\n",
      "Iteration:  30\n",
      "[1 8 9 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.1951864406779661\n",
      "Iteration:  40\n",
      "[1 8 5 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.21894915254237288\n",
      "Iteration:  50\n",
      "[6 8 5 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.23535593220338982\n",
      "Iteration:  60\n",
      "[6 1 5 ... 5 1 8] [3 3 5 ... 0 2 4]\n",
      "0.25245762711864406\n",
      "Iteration:  70\n",
      "[6 1 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.27208474576271185\n",
      "Iteration:  80\n",
      "[6 1 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.2931186440677966\n",
      "Iteration:  90\n",
      "[3 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.32266101694915256\n",
      "Iteration:  100\n",
      "[3 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.3717966101694915\n",
      "Iteration:  110\n",
      "[3 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.40147457627118643\n",
      "Iteration:  120\n",
      "[3 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.4302881355932203\n",
      "Iteration:  130\n",
      "[3 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.46547457627118644\n",
      "Iteration:  140\n",
      "[5 3 6 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.49047457627118646\n",
      "Iteration:  150\n",
      "[5 3 6 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.5146610169491526\n",
      "Iteration:  160\n",
      "[5 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.5377796610169492\n",
      "Iteration:  170\n",
      "[5 3 5 ... 5 1 0] [3 3 5 ... 0 2 4]\n",
      "0.5603389830508475\n",
      "Iteration:  180\n",
      "[5 3 5 ... 5 1 2] [3 3 5 ... 0 2 4]\n",
      "0.5826440677966102\n",
      "Iteration:  190\n",
      "[5 3 5 ... 5 1 3] [3 3 5 ... 0 2 4]\n",
      "0.6038813559322034\n",
      "Iteration:  200\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.6231525423728813\n",
      "Iteration:  210\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.6417796610169492\n",
      "Iteration:  220\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.6575762711864407\n",
      "Iteration:  230\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.6725254237288135\n",
      "Iteration:  240\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.6850338983050848\n",
      "Iteration:  250\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.697728813559322\n",
      "Iteration:  260\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7089830508474576\n",
      "Iteration:  270\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7193728813559322\n",
      "Iteration:  280\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7285932203389831\n",
      "Iteration:  290\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7372542372881355\n",
      "Iteration:  300\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7443728813559322\n",
      "Iteration:  310\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7514406779661017\n",
      "Iteration:  320\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7584915254237288\n",
      "Iteration:  330\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7644406779661017\n",
      "Iteration:  340\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7702203389830509\n",
      "Iteration:  350\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.775135593220339\n",
      "Iteration:  360\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7801694915254237\n",
      "Iteration:  370\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7846271186440678\n",
      "Iteration:  380\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7886610169491526\n",
      "Iteration:  390\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7923559322033898\n",
      "Iteration:  400\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.7966610169491526\n",
      "Iteration:  410\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8004237288135593\n",
      "Iteration:  420\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8037457627118644\n",
      "Iteration:  430\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.806728813559322\n",
      "Iteration:  440\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.809322033898305\n",
      "Iteration:  450\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8119152542372882\n",
      "Iteration:  460\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8147457627118644\n",
      "Iteration:  470\n",
      "[5 3 5 ... 5 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8174576271186441\n",
      "Iteration:  480\n",
      "[5 3 5 ... 0 2 4] [3 3 5 ... 0 2 4]\n",
      "0.819728813559322\n",
      "Iteration:  490\n",
      "[5 3 5 ... 0 2 4] [3 3 5 ... 0 2 4]\n",
      "0.8226949152542373\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f45be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
